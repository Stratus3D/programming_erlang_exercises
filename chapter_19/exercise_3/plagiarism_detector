#!/usr/bin/env escript
% -d

-include_lib("kernel/include/file.hrl").

print_usage() ->
  io:format("plagiarism_detector text_files/").

main([DirectoryName]) ->
    % List files in text_files directory
    {ok, Filenames} = file:list_dir(DirectoryName),

    % Write filenames to ETS table
    FilenameTable = ets:new(filenames, []),
    lists:foldl(fun(Filename, Index) ->
                          FullFilename = filename:join([[DirectoryName, "/"], Filename]),
                          ets:insert(FilenameTable, {Index, FullFilename}),
                          Index + 1
                  end, 1, Filenames),

    % Read all files and compute hashes for each 40 bytes, write each hash to another ETS
    HashesTable = ets:new(hashes, [duplicate_bag]),
    ets:foldl(fun(FileRecord, _) ->
                      process_file(FileRecord, HashesTable)
              end, [], FilenameTable),

    % Retrieve all hashes from table
    % Probably not the intended way to solve this exercise, but this turned out
    % to be the easiest way to do this.
    Records = ets:tab2list(HashesTable),

    % Find duplicate hashes
    Duplicates = find_duplicate_hashes(Records),
    io:format("length: ~p~n", [length(Records)]),
    io:format("Duplicates: ~p~n", [Duplicates]),
    ok;

main(_) ->
  % Print usage message if invalid number of arguments is passed in
  print_usage(),
  halt(1).

process_file({FileID, Filename}, HashesTable) ->
    compute_hashes(Filename, FileID, 0, HashesTable).

compute_hashes(Filename, FileID, Offset, HashesTable) ->
    case file:read_file(Filename) of
        {ok, Data} ->
            Hashes = rolling_hash(Data),
            lists:foldl(fun(Hash, Index) ->
                                ets:insert(HashesTable, {Hash, {FileID, Index}}),
                                Index + 1
                        end, 0, Hashes);
        eof ->
            ok
    end.

find_duplicate_hashes(Records) ->
    lists:foldl(fun({Hash, {FileID, Offset}}, Acc) ->
                         case lists:filter(fun({SecondHash, {SecondFileID, _}}) ->
                                              (Hash =:= SecondHash) and (FileID =/= SecondFileID)
                                      end, Records) of
                             [] ->
                                 Acc;
                             Values ->
                                 Processed = lists:map(fun({Hash, {FileID, Offset}}) ->
                                                               {FileID, Offset}
                                                               end, Values),
                                 [{Hash, [{FileID, Offset}|Processed]}|Acc]
                         end
                 end, [], Records).

% Probably not the best way to implement a rolling hash, but I was short on time
rolling_hash(Data) ->
    % Take the first 40 bytes
    <<Chunk:40/binary, Rest/binary>> = Data,

    % Compute the hash for the first chunk
    Hash = hash_string(Chunk),

    % Compute all the following hashes
    [Hash|compute_next_hash(Hash, Chunk, Rest)].

compute_next_hash(LastHash, <<LastByte:1/binary, Chunk/binary>>, <<>>) ->
    [hash_string(LastByte, LastHash, <<>>)];
compute_next_hash(LastHash, <<LastByte:1/binary, Chunk/binary>>, <<NextByte:1/binary, NewRest/binary>>) ->
    % Add one byte to the end, remove one byte from the beginning and repeat
    Hash = hash_string(LastByte, LastHash, NextByte),
    [Hash|compute_next_hash(Hash, <<Chunk/binary, NextByte/binary>>, NewRest)].

% String to integer hash
hash_string(String) ->
    Integers = [Byte || <<Byte>> <= String],
    lists:foldl(fun(X, Sum) -> X + Sum end, 0, Integers).

% Hash plus new minus old
hash_string(Old, Hash, New) ->
    OldInt = hash_byte(Old),
    NewInt = hash_byte(New),
    % Remove old character
    (Hash - OldInt) + NewInt.

% Byte to integer
hash_byte(<<Integer>>) ->
    Integer;
hash_byte(<<>>) ->
    0.
